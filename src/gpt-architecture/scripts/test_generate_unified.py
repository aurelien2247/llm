"""
Test de la fonction generate() unifiÃ©e (suivant le tutoriel).

Ce script teste la nouvelle fonction generate() qui combine:
- Temperature scaling
- Top-k sampling
- Early stopping

Reproduit l'exemple du tutoriel:
    generate(model, idx, max_new_tokens=15, context_size=..., top_k=25, temperature=1.4)

Utilisation:
    python3 test_generate_unified.py
"""

import sys
import os
import torch
import tiktoken

sys.path.insert(0, '..')

from config import GPT_CONFIG_124M, PATHS
from core import GPTModel, text_to_token_ids, token_ids_to_text
from messages import model_loaded_from, error_not_found, MISSING_CHECKPOINT_HELP
from decoding import generate


def main():
    """Test de la fonction generate() unifiÃ©e."""
    
    print("=" * 80)
    print("TEST DE LA FONCTION generate() UNIFIÃ‰E")
    print("=" * 80)
    print()
    
    # Charger le modÃ¨le
    print("ğŸ“¦ Chargement du modÃ¨le...")
    device = torch.device("cpu")
    model = GPTModel(GPT_CONFIG_124M)

    # Plusieurs chemins candidats selon d'oÃ¹ on exÃ©cute le script
    candidates = [
        os.path.join("..", PATHS["model_checkpoint"]),        # ../gpt-model.pt (attendu depuis src/gpt-architecture/scripts)
        PATHS["model_checkpoint"],                              # gpt-model.pt (cwd)
        os.path.join(".", PATHS["model_checkpoint"]),         # ./gpt-model.pt
        os.path.join("..", "scripts", PATHS["model_checkpoint"]),
        os.path.join("scripts", PATHS["model_checkpoint"]),
    ]

    loaded = False
    for p in candidates:
        try:
            if os.path.exists(p):
                model.load_state_dict(torch.load(p, map_location=device))
                model.to(device)
                model.eval()
                print(model_loaded_from(p))
                loaded = True
                break
        except Exception:
            # Si torch.load Ã©choue pour une autre raison, continuer et afficher l'erreur plus bas
            continue

    if not loaded:
        print(error_not_found(PATHS['model_checkpoint']) + f" (recherchÃ©: {', '.join(candidates)})")
        print(MISSING_CHECKPOINT_HELP + " ou placer le checkpoint dans l'un des chemins ci-dessus.")
        return
    
    # Configuration
    tokenizer = tiktoken.get_encoding("gpt2")
    start_context = "Every effort moves you"
    
    print(f"Contexte initial: \"{start_context}\"")
    print(f"Max tokens: 15")
    print()
    
    # Encoder le contexte
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    
    # Test 1: Greedy (baseline)
    print("-" * 80)
    print("TEST 1: GREEDY DECODING (baseline)")
    print("-" * 80)
    print("ParamÃ¨tres: temperature=0.0 (dÃ©sactivÃ©), top_k=None")
    print()
    
    torch.manual_seed(123)
    token_ids = generate(
        model=model,
        idx=encoded,
        max_new_tokens=15,
        context_size=GPT_CONFIG_124M["context_length"],
        temperature=0.0,  # Greedy
        top_k=None
    )
    text = token_ids_to_text(token_ids, tokenizer)
    print(f"RÃ©sultat: {text}")
    print()
    
    # Test 2: Temperature seule
    print("-" * 80)
    print("TEST 2: TEMPERATURE SCALING SEUL")
    print("-" * 80)
    print("ParamÃ¨tres: temperature=1.4, top_k=None")
    print("Effet: Plus de variÃ©tÃ©, mais risque de tokens absurdes")
    print()
    
    torch.manual_seed(123)
    token_ids = generate(
        model=model,
        idx=encoded,
        max_new_tokens=15,
        context_size=GPT_CONFIG_124M["context_length"],
        temperature=1.4,
        top_k=None
    )
    text = token_ids_to_text(token_ids, tokenizer)
    print(f"RÃ©sultat: {text}")
    print()
    
    # Test 3: Top-k seul
    print("-" * 80)
    print("TEST 3: TOP-K SAMPLING SEUL")
    print("-" * 80)
    print("ParamÃ¨tres: temperature=0.0 (greedy sur top-k), top_k=25")
    print("Effet: Ã‰limine tokens absurdes, mais reste dÃ©terministe")
    print()
    
    torch.manual_seed(123)
    token_ids = generate(
        model=model,
        idx=encoded,
        max_new_tokens=15,
        context_size=GPT_CONFIG_124M["context_length"],
        temperature=0.0,  # Greedy dans le top-k
        top_k=25
    )
    text = token_ids_to_text(token_ids, tokenizer)
    print(f"RÃ©sultat: {text}")
    print()
    
    # Test 4: Temperature + Top-k (COMME LE TUTORIEL)
    print("-" * 80)
    print("TEST 4: TEMPERATURE + TOP-K (TUTORIEL)")
    print("-" * 80)
    print("ParamÃ¨tres: temperature=1.4, top_k=25")
    print("Effet: VariÃ©tÃ© contrÃ´lÃ©e + Ã©limination tokens absurdes")
    print()
    
    torch.manual_seed(123)
    token_ids = generate(
        model=model,
        idx=encoded,
        max_new_tokens=15,
        context_size=GPT_CONFIG_124M["context_length"],
        temperature=1.4,
        top_k=25
    )
    text = token_ids_to_text(token_ids, tokenizer)
    print(f"RÃ©sultat: {text}")
    print()
    print("ğŸ’¡ Ce texte est diffÃ©rent du texte mÃ©morisÃ© gÃ©nÃ©rÃ© par greedy!")
    print("   Il montre que le modÃ¨le peut gÃ©nÃ©rer du contenu crÃ©atif.")
    print()
    
    # Test 5: DiffÃ©rentes graines
    print("-" * 80)
    print("TEST 5: VARIÃ‰TÃ‰ AVEC DIFFÃ‰RENTES GRAINES")
    print("-" * 80)
    print("ParamÃ¨tres: temperature=1.4, top_k=25, 3 graines diffÃ©rentes")
    print()
    
    for seed in [42, 123, 456]:
        torch.manual_seed(seed)
        token_ids = generate(
            model=model,
            idx=encoded,
            max_new_tokens=15,
            context_size=GPT_CONFIG_124M["context_length"],
            temperature=1.4,
            top_k=25
        )
        text = token_ids_to_text(token_ids, tokenizer)
        print(f"Graine {seed}: {text}")
    
    print()
    
    # Analyse comparative
    print("=" * 80)
    print("ANALYSE COMPARATIVE")
    print("=" * 80)
    print()
    
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Configuration            â”‚ DÃ©terministeâ”‚ QualitÃ©                  â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ Greedy (T=0, k=None)     â”‚     âœ“       â”‚ MÃ©morisÃ©, rÃ©pÃ©titif      â”‚")
    print("â”‚ Temperature (T=1.4)      â”‚     âœ—       â”‚ VariÃ© mais risquÃ©        â”‚")
    print("â”‚ Top-k (k=25)             â”‚     âœ“       â”‚ SÃ»r mais dÃ©terministe    â”‚")
    print("â”‚ T=1.4 + k=25 â­          â”‚     âœ—       â”‚ VariÃ© ET contrÃ´lÃ©        â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    
    print("ğŸ¯ CONCLUSION:")
    print("  La combinaison temperature + top-k offre le meilleur compromis:")
    print("  â€¢ Temperature â†’ variÃ©tÃ© et crÃ©ativitÃ©")
    print("  â€¢ Top-k â†’ Ã©limination des tokens absurdes")
    print("  â€¢ RÃ©sultat â†’ texte intÃ©ressant ET cohÃ©rent")
    print()
    
    print("ğŸ’¡ RECOMMANDATIONS:")
    print("  â€¢ QA/PrÃ©cision:  temperature=0.0, top_k=None (greedy)")
    print("  â€¢ QualitÃ©:       temperature=0.7, top_k=50")
    print("  â€¢ Ã‰quilibre:     temperature=1.0, top_k=40")
    print("  â€¢ CrÃ©atif:       temperature=1.4, top_k=25  â­ (tutoriel)")
    print()


if __name__ == "__main__":
    main()
