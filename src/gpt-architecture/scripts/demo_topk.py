"""
DÃ©monstration du Top-K Sampling (suivant le tutoriel).

Ce script illustre comment le top-k sampling amÃ©liore la gÃ©nÃ©ration de texte
en Ã©liminant les tokens peu probables qui causent du texte absurde.

Concepts dÃ©montrÃ©s:
1. ProblÃ¨me: temperature seule peut gÃ©nÃ©rer des tokens absurdes
2. Solution: top-k garde seulement les k tokens les plus probables
3. RÃ©sultat: diversitÃ© conservÃ©e, mais sans tokens complÃ¨tement incohÃ©rents

Utilisation:
    python3 demo_topk.py
"""

import sys
import torch

sys.path.insert(0, '..')


def main():
    """DÃ©monstration du top-k sampling Ã©tape par Ã©tape."""
    
    print("=" * 80)
    print("DÃ‰MONSTRATION : TOP-K SAMPLING")
    print("=" * 80)
    print()
    
    # Vocabulaire d'exemple (du tutoriel)
    vocab = {
        "closer": 0,
        "every": 1,
        "effort": 2,
        "forward": 3,
        "inches": 4,
        "moves": 5,
        "pizza": 6,  # Token absurde dans ce contexte !
        "toward": 7,
        "you": 8,
    }
    
    inverse_vocab = {v: k for k, v in vocab.items()}
    
    # Logits du modÃ¨le (du tutoriel)
    next_token_logits = torch.tensor(
        [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
    )
    
    print("ğŸ“Š LOGITS BRUTS DU MODÃˆLE")
    print("-" * 80)
    for word, logit_val in zip(vocab.keys(), next_token_logits.tolist()):
        print(f"  {word:12s}: {logit_val:7.2f}")
    print()
    
    # Ã‰tape 1: ProbabilitÃ©s sans filtrage (avec temperature=1.0)
    print("ğŸ“Š Ã‰TAPE 1: PROBABILITÃ‰S SANS FILTRAGE (tempÃ©rature=1.0)")
    print("-" * 80)
    probas_no_filter = torch.softmax(next_token_logits, dim=0)
    
    for idx, (word, prob) in enumerate(zip(vocab.keys(), probas_no_filter.tolist())):
        bar = "â–ˆ" * int(prob * 100)
        print(f"  {word:12s}: {prob:6.4f} {bar}")
    print()
    print("âš ï¸  PROBLÃˆME: 'pizza' a 0.0018 de probabilitÃ© (0.18%)")
    print("    Avec temperature Ã©levÃ©e, peut Ãªtre sÃ©lectionnÃ© â†’ texte absurde!")
    print()
    
    # Ã‰tape 2: SÃ©lectionner les top-k logits
    print("ğŸ“Š Ã‰TAPE 2: SÃ‰LECTIONNER LES TOP-K LOGITS (k=3)")
    print("-" * 80)
    top_k = 3
    top_logits, top_pos = torch.topk(next_token_logits, top_k)
    
    print(f"Top {top_k} logits: {top_logits.tolist()}")
    print(f"Top {top_k} positions: {top_pos.tolist()}")
    print()
    
    print("Tokens correspondants:")
    for pos, logit in zip(top_pos.tolist(), top_logits.tolist()):
        word = inverse_vocab[pos]
        print(f"  {word:12s}: {logit:7.2f}")
    print()
    
    # Ã‰tape 3: Masquer les logits hors du top-k
    print("ğŸ“Š Ã‰TAPE 3: MASQUER LES TOKENS HORS DU TOP-K")
    print("-" * 80)
    print("MÃ©thode: mettre Ã  -inf tous les logits < plus petit top-k logit")
    print()
    
    # Utiliser torch.where pour masquer (comme dans le tutoriel)
    new_logits = torch.where(
        condition=next_token_logits < top_logits[-1],  # Condition: logit < min(top-k)
        input=torch.tensor(float("-inf")),              # Si vrai: -inf (masquÃ©)
        other=next_token_logits                         # Sinon: garder logit original
    )
    
    print("Logits aprÃ¨s masquage:")
    for word, logit in zip(vocab.keys(), new_logits.tolist()):
        if logit == float("-inf"):
            print(f"  {word:12s}: -inf (MASQUÃ‰)")
        else:
            print(f"  {word:12s}: {logit:7.2f}")
    print()
    
    # Ã‰tape 4: Appliquer softmax pour obtenir les probabilitÃ©s
    print("ğŸ“Š Ã‰TAPE 4: APPLIQUER SOFTMAX (renormalisation)")
    print("-" * 80)
    topk_probas = torch.softmax(new_logits, dim=0)
    
    print("ProbabilitÃ©s aprÃ¨s top-k sampling:")
    total_prob = 0.0
    for word, prob in zip(vocab.keys(), topk_probas.tolist()):
        if prob > 0:
            bar = "â–ˆ" * int(prob * 100)
            print(f"  {word:12s}: {prob:6.4f} {bar}")
            total_prob += prob
        else:
            print(f"  {word:12s}: 0.0000 (Ã©liminÃ©)")
    
    print()
    print(f"âœ“ Somme des probabilitÃ©s: {total_prob:.6f}")
    print()
    
    # Comparaison avant/aprÃ¨s
    print("=" * 80)
    print("COMPARAISON AVANT/APRÃˆS")
    print("=" * 80)
    print()
    
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚                      SANS TOP-K         â”‚        AVEC TOP-K (k=3)   â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for word in vocab.keys():
        idx = vocab[word]
        prob_before = probas_no_filter[idx].item()
        prob_after = topk_probas[idx].item()
        
        bar_before = "â–ˆ" * int(prob_before * 50)
        bar_after = "â–ˆ" * int(prob_after * 50)
        
        status = ""
        if prob_after == 0:
            status = "âœ— Ã‰LIMINÃ‰"
        elif prob_after > prob_before * 1.5:
            status = "â†‘ BOOSTÃ‰"
        
        print(f"â”‚ {word:10s} {prob_before:6.4f} {bar_before:20s}â”‚ {prob_after:6.4f} {bar_after:20s} {status:10s} â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    
    # Analyse des rÃ©sultats
    print("=" * 80)
    print("ANALYSE DES RÃ‰SULTATS")
    print("=" * 80)
    print()
    
    print("âœ… AVANTAGES DU TOP-K SAMPLING:")
    print("  â€¢ Ã‰limine les tokens absurdes ('pizza', 'effort', etc.)")
    print("  â€¢ Conserve les tokens cohÃ©rents ('forward', 'toward', 'inches')")
    print("  â€¢ RÃ©duit le risque de gÃ©nÃ©ration incohÃ©rente")
    print("  â€¢ Peut Ãªtre combinÃ© avec temperature scaling")
    print()
    
    print("ğŸ“Š EFFETS OBSERVÃ‰S:")
    pizza_before = probas_no_filter[vocab["pizza"]].item()
    pizza_after = topk_probas[vocab["pizza"]].item()
    print(f"  â€¢ 'pizza' : {pizza_before:.4f} â†’ {pizza_after:.4f} (Ã©liminÃ©!)")
    
    forward_before = probas_no_filter[vocab["forward"]].item()
    forward_after = topk_probas[vocab["forward"]].item()
    print(f"  â€¢ 'forward': {forward_before:.4f} â†’ {forward_after:.4f} (boostÃ©!)")
    print()
    
    print("ğŸ’¡ RECOMMANDATIONS:")
    print("  â€¢ k=3-10   : TrÃ¨s conservateur, texte cohÃ©rent mais rÃ©pÃ©titif")
    print("  â€¢ k=30-50  : Ã‰quilibre qualitÃ©/variÃ©tÃ© (RECOMMANDÃ‰)")
    print("  â€¢ k=100+   : Plus de variÃ©tÃ©, risque de tokens peu pertinents")
    print()
    
    print("ğŸ”§ COMBINAISON AVEC TEMPERATURE:")
    print("  â€¢ top_k=50 + temperature=0.7 : GÃ©nÃ©ration de qualitÃ©")
    print("  â€¢ top_k=30 + temperature=1.0 : Ã‰quilibre")
    print("  â€¢ top_k=100 + temperature=1.5 : CrÃ©atif mais contrÃ´lÃ©")
    print()


if __name__ == "__main__":
    main()
