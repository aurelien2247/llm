# Architecture GPT - Impl√©mentation compl√®te

Ce dossier contient une impl√©mentation compl√®te d'un mod√®le GPT-124M avec capacit√©s d'entra√Ænement, d'√©valuation et de g√©n√©ration de texte.

## üìã Vue d'ensemble

Le projet impl√©mente l'architecture Transformer suivant les principes du livre "Build a Large Language Model from Scratch" (Sebastian Raschka).

### Caract√©ristiques principales
- **Tokenization flexible** : Support de tiktoken (BPE) et tokenizers personnalis√©s
- **Pipeline de donn√©es optimis√©** : Fen√™tres glissantes avec support de tokenizers multiples
- **Boucle d'entra√Ænement compl√®te** : Entra√Ænement, validation et g√©n√©ration de texte
- **Gestion des checkpoints** : Sauvegarde et chargement du mod√®le entra√Æn√©
- **G√©n√©ration de texte** : G√©n√©ration autonome avec control de contexte

## üìÅ Structure des fichiers

### Modules core
- **`model.py`** - Classe GPTModel : architecture compl√®te du mod√®le
- **`layers.py`** - Composants des couches (LayerNorm, GELU, FeedForward, TransformerBlock)
- **`attention.py`** - Impl√©mentation de l'attention multi-t√™tes avec masque causal
- **`generate.py`** - Fonction de g√©n√©ration simple de texte

### Pipeline de donn√©es et pr√©traitement
- **`tokenizer.py`** - SimpleTokenizerV1/V2 pour la tokenization personnalis√©e
- **`data.py`** - GPTDatasetV1 et create_dataloader_v1 avec support multi-tokenizer
- **`utils.py`** - Utilitaires : conversion texte/tokens, calcul de perte

### Scripts d'entra√Ænement et inf√©rence
- **`training.py`** - Fonctions utilitaires d'entra√Ænement (train_model_simple, evaluate_model, etc.)
- **`train.py`** - Script complet d'entra√Ænement avec gestion des checkpoints
- **`main.py`** - Script de d√©monstration simple avec g√©n√©ration de texte
- **`load_model.py`** - Script pour charger un mod√®le entra√Æn√© et g√©n√©rer du texte

## üöÄ Utilisation

### Installation des d√©pendances
```bash
cd /Users/moignet/Projects/llm
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Entra√Ænement du mod√®le
```bash
python3 src/gpt-architecture/train.py
```

Cela va :
- Charger le texte depuis `data/the-verdict.txt`
- Cr√©er les dataloaders train/val (90/10 split)
- Initialiser le mod√®le GPT-124M
- Entra√Æner pendant 10 epochs avec √©valuation tous les 5 epochs
- Sauvegarder le checkpoint dans `gpt-model.pt`
- Afficher les courbes de perte (si matplotlib est disponible)

### G√©n√©ration de texte avec un mod√®le entra√Æn√©
```bash
python3 src/gpt-architecture/load_model.py --model_path gpt-model.pt --prompt "Hello, I" --max_tokens 100
```

### D√©monstration simple (mod√®le non entra√Æn√©)
```bash
python3 src/gpt-architecture/main.py
```

## üß† Configuration du mod√®le

La configuration par d√©faut (GPT_CONFIG_124M) :
```python
{
    "vocab_size": 50257,        # Taille du vocabulaire GPT-2
    "context_length": 1024,     # Longueur maximale du contexte
    "emb_dim": 768,             # Dimension des embeddings
    "n_heads": 12,              # Nombre de t√™tes d'attention
    "n_layers": 12,             # Nombre de blocs Transformer
    "drop_rate": 0.1,           # Taux de dropout
    "qkv_bias": False           # Biais pour QKV
}
```

## üìä R√©sultats d'entra√Ænement

Le mod√®le entra√Æn√© sur le texte "The Verdict" montre :
- **Perte d'entra√Ænement initial** : 9.787
- **Perte d'entra√Ænement final** : 1.314
- **Convergence** : Progressive sur 10 epochs
- **Qualit√© de g√©n√©ration** : Du texte gibberish au texte presque naturel

## üîß Architecture d√©taill√©e

### Flux forward
```
Entr√©e (token IDs)
    ‚Üì
Embedding tokens + positions
    ‚Üì
Dropout
    ‚Üì
N couches Transformer (attention + feed-forward)
    ‚Üì
LayerNorm
    ‚Üì
Projection de sortie
    ‚Üì
Logits (vocab_size)
```

### Attention multi-t√™tes
- Division en 12 t√™tes (768 / 12 = 64 dim par t√™te)
- Masque causal pour pr√©venir l'attention sur les jetons futurs
- Softmax avec facteur d'√©chelle

### Bloc Transformer
```
Attention multi-t√™tes
    ‚Üì + connexion r√©sidu
LayerNorm
    ‚Üì
Feed-Forward (MLP)
    ‚Üì + connexion r√©sidu
```

## üìö R√©f√©rences

- "Build a Large Language Model from Scratch" - Sebastian Raschka
- [OpenAI GPT-2](https://openai.com/blog/better-language-models/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al.

## üìù Notes d'impl√©mentation

### Tokenization flexible
Le pipeline supporte deux approches :
1. **tiktoken** (par d√©faut) : Tokenization BPE compatible OpenAI
2. **SimpleTokenizerV2** : Tokenizer personnalis√© pour exp√©rimentation p√©dagogique

### Gestion des erreurs
Le code inclut une gestion robuste :
- Try/except pour la compatibility des tokenizers
- Fallback gracieux pour matplotlib (si non install√©)
- Gestion des appareils (CPU/GPU)

## ‚ö° Performance

- **Dur√©e d'entra√Ænement** : ~5 minutes (10 epochs) sur CPU
- **Taille du checkpoint** : ~621 MB (mod√®le seul)
- **M√©moire requise** : ~2 GB pour entra√Ænement

## ü§ù Cr√©dits

Impl√©mentation cr√©√©e pour apprentissage pratique de l'architecture GPT et du deep learning.
Bas√©e sur les principes p√©dagogiques du livre de Sebastian Raschka.
