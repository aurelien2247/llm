# ğŸ—ï¸ Architecture GPT - ImplÃ©mentation Professionnelle

Une implÃ©mentation complÃ¨te et bien structurÃ©e d'un modÃ¨le GPT-124M avec capacitÃ©s d'entraÃ®nement, d'Ã©valuation et de gÃ©nÃ©ration de texte. ConÃ§u comme POC pour dÃ©montrer la faisabilitÃ© d'une implÃ©mentation LLM from scratch en production.

## ğŸ“Š Vue d'ensemble

- **ModÃ¨le**: GPT-124M (Transformer decoder-only)
- **Tokenization**: tiktoken (BPE) + support tokenizers personnalisÃ©s
- **Framework**: PyTorch avec architecture modulaire
- **StratÃ©gies de gÃ©nÃ©ration**: greedy, temperature, top-k, top-p (nucleus)
- **Documentation**: ComplÃ¨te en franÃ§ais avec docstrings

## ğŸ›ï¸ Architecture du Projet

```
gpt-architecture/
â”œâ”€â”€ core/                          # âš™ï¸ Architecture du modÃ¨le
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # GPTModel (classe principale)
â”‚   â”œâ”€â”€ layers.py                  # LayerNorm, GELU, FeedForward, TransformerBlock
â”‚   â”œâ”€â”€ attention.py               # MultiHeadAttention avec masque causal
â”‚   â””â”€â”€ utils.py                   # Utilitaires (text_to_token_ids, calc_loss_*)
â”‚
â”œâ”€â”€ data/                          # ğŸ“¦ Pipeline de donnÃ©es
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                  # GPTDatasetV1, create_dataloader_v1
â”‚   â””â”€â”€ tokenizer.py               # SimpleTokenizerV1, SimpleTokenizerV2
â”‚
â”œâ”€â”€ decoding/                      # ğŸ² StratÃ©gies de gÃ©nÃ©ration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ strategies.py              # softmax_avec_temperature, top_k, top_p, etc.
â”‚   â””â”€â”€ generator.py               # Fonctions generate_text_*
â”‚
â”œâ”€â”€ scripts/                       # ğŸš€ ExÃ©cutables
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                   # EntraÃ®nement complet du modÃ¨le
â”‚   â”œâ”€â”€ infer.py                   # InfÃ©rence avec diffÃ©rentes stratÃ©gies
â”‚   â”œâ”€â”€ demo.py                    # DÃ©monstration des stratÃ©gies cÃ´te Ã  cÃ´te
â”‚   â””â”€â”€ visualize.py               # Visualisation de tempÃ©rature (PDF)
â”‚
â”œâ”€â”€ training.py                    # ğŸ“š Utilitaires d'entraÃ®nement (train_model_simple, etc.)
â”œâ”€â”€ config.py                      # âš™ï¸ Configuration centralisÃ©e
â”œâ”€â”€ README.md                      # ğŸ“– Ce fichier
â””â”€â”€ *.pdf                          # Sorties graphiques (ignorÃ©es par git)
```

## ğŸ”„ HiÃ©rarchie des dÃ©pendances

```
scripts/  â† Point d'entrÃ©e utilisateur
    â†“
training.py
    â†“
core + data + decoding
    â†“
PyTorch + tiktoken
```

## âœ¨ CaractÃ©ristiques principales

### ğŸ” ModularitÃ©
- **SÃ©paration des responsabilitÃ©s**: core (modÃ¨le), data (donnÃ©es), decoding (gÃ©nÃ©ration), scripts (exÃ©cution)
- **Imports explicites**: Facilement repÃ©rable oÃ¹ vient chaque fonction
- **Configuration centralisÃ©e**: `config.py` pour tous les hyperparamÃ¨tres

### ğŸ§  Architecture Transformer
- Embedding + positional encoding
- Multi-head attention avec masque causal
- Couches Feed-Forward (expansion 4x)
- Layer normalization + rÃ©sidus
- 12 couches Ã— 12 tÃªtes (768 dim)

### ğŸ“Š Pipeline de donnÃ©es
- FenÃªtres glissantes (sliding windows)
- Support tiktoken (BPE) et tokenizers personnalisÃ©s
- Train/val split configurable (dÃ©faut: 90/10)
- DataLoader PyTorch standard

### ğŸ›ï¸ StratÃ©gies de gÃ©nÃ©ration
| StratÃ©gie | CohÃ©rence | VariÃ©tÃ© | Cas d'usage |
|-----------|-----------|---------|------------|
| **Greedy** | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ | QA prÃ©cis |
| **Temperature 0.3** | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ | Texte contrÃ´lÃ© |
| **Temperature 1.0** | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | Usage gÃ©nÃ©ral |
| **Top-k (k=50)** | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | Ã‰quilibre |
| **Top-p (p=0.9)** | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | â­ RecommandÃ© |

### ğŸ“ˆ Monitoring & Visualisation
- Suivi des losses train/val
- GÃ©nÃ©ration d'exemples tous les epochs
- Graphique PDF des courbes de perte
- Visualisation temperature effects (PDF)

## ğŸš€ Guide rapide

### Installation

```bash
cd /Users/moignet/Projects/llm
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 1ï¸âƒ£ EntraÃ®ner le modÃ¨le

```bash
cd src/gpt-architecture/scripts
python3 train.py
```

**RÃ©sultat**: CrÃ©e `gpt-model.pt` et `loss-plot.pdf`

### 2ï¸âƒ£ GÃ©nÃ©rer du texte (infÃ©rence)

```bash
# Greedy (dÃ©terministe)
python3 infer.py --strategy greedy --prompt "Every effort" --max_tokens 50

# TempÃ©rature (variÃ©tÃ© contrÃ´lÃ©e)
python3 infer.py --strategy temperature --temperature 0.7 --max_tokens 50

# Top-k (Ã©vite les tokens absurdes)
python3 infer.py --strategy top_k --k 50 --max_tokens 50

# Top-p (nucleus sampling)
python3 infer.py --strategy top_p --p 0.9 --max_tokens 50
```

### 3ï¸âƒ£ Comparer les stratÃ©gies

```bash
python3 demo.py
```

Affiche 6 variantes cÃ´te Ã  cÃ´te avec analyses.

### 4ï¸âƒ£ Visualiser l'effet tempÃ©rature

```bash
python3 visualize.py
```

GÃ©nÃ¨re `temperature-visualization.pdf`.
```bash
python3 src/gpt-architecture/main.py
```

## ğŸ§  Configuration du modÃ¨le

La configuration par dÃ©faut (GPT_CONFIG_124M) :
```python
{
    "vocab_size": 50257,        # Taille du vocabulaire GPT-2
    "context_length": 1024,     # Longueur maximale du contexte
    "emb_dim": 768,             # Dimension des embeddings
    "n_heads": 12,              # Nombre de tÃªtes d'attention
    "n_layers": 12,             # Nombre de blocs Transformer
    "drop_rate": 0.1,           # Taux de dropout
    "qkv_bias": False           # Biais pour QKV
}
```

## ğŸ“Š RÃ©sultats d'entraÃ®nement

Le modÃ¨le entraÃ®nÃ© sur le texte "The Verdict" montre :
- **Perte d'entraÃ®nement initial** : 9.787
- **Perte d'entraÃ®nement final** : 1.314
- **Convergence** : Progressive sur 10 epochs
- **QualitÃ© de gÃ©nÃ©ration** : Du texte gibberish au texte presque naturel

## ğŸ›ï¸ StratÃ©gies de dÃ©codage

Le projet implÃ©mente 4 stratÃ©gies pour contrÃ´ler l'alÃ©atoire lors de la gÃ©nÃ©ration:

### 1. Greedy Decoding (argmax)
SÃ©lectionne le token avec la plus haute probabilitÃ© Ã  chaque Ã©tape.
- âœ… DÃ©terministe et reproductible
- âœ… Texte cohÃ©rent
- âŒ Peu de variÃ©tÃ©

### 2. Temperature Scaling
Applique un scaling aux logits avant softmax pour contrÃ´ler la "confiance" du modÃ¨le.
- **Temperature < 1** (ex: 0.3): Distribution plus nette â†’ texte plus cohÃ©rent
- **Temperature = 1**: Pas de scaling â†’ comportement normal
- **Temperature > 1** (ex: 2.0): Distribution plus plate â†’ plus d'alÃ©atoire

Formule: `scaled_logits = logits / temperature`

### 3. Top-k Sampling
Garde seulement les k tokens les plus probables et Ã©limine le reste.
- âœ… Ã‰vite les tokens absurdes
- âœ… Meilleure qualitÃ© que temperature seul
- âœ“ Nombre de tokens constant

Exemple: k=50 garde les 50 tokens les plus probables

### 4. Top-p (Nucleus) Sampling
Garde les tokens dont la probabilitÃ© cumulÃ©e atteint p (ex: 90%).
- âœ… Flexible: ajuste le nombre de tokens selon la distribution
- âœ… Bonne qualitÃ© et variÃ©tÃ©
- âœ“ Adapte le niveau de contrÃ´le dynamiquement

Exemple: p=0.9 garde les tokens reprÃ©sentant 90% de la masse de probabilitÃ©

### Comparaison et recommandations

| StratÃ©gie | CohÃ©rence | VariÃ©tÃ© | Cas d'usage |
|-----------|-----------|---------|------------|
| Greedy | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ | QA prÃ©cis |
| T=0.3 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ | Texte prÃ©cis |
| T=1.0 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | Ã‰quilibre |
| T=2.0 | â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | CrÃ©ativitÃ© |
| Top-k | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | Ã‰quilibre |
| Top-p | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ | RecommandÃ© |

**Recommandations:**
- **QA/PrÃ©cision**: temperature=0.1-0.3 ou greedy
- **Usage gÃ©nÃ©ral**: temperature=0.7-1.0 ou top-k/top-p
- **CrÃ©ativitÃ©**: temperature=1.5-2.0 ou top-p (p=0.95)

## ğŸ”§ Architecture dÃ©taillÃ©e

### Flux forward
```
EntrÃ©e (token IDs)
    â†“
Embedding tokens + positions
    â†“
Dropout
    â†“
N couches Transformer (attention + feed-forward)
    â†“
LayerNorm
    â†“
Projection de sortie
    â†“
Logits (vocab_size)
```

### Attention multi-tÃªtes
- Division en 12 tÃªtes (768 / 12 = 64 dim par tÃªte)
- Masque causal pour prÃ©venir l'attention sur les jetons futurs
- Softmax avec facteur d'Ã©chelle

### Bloc Transformer
```
Attention multi-tÃªtes
    â†“ + connexion rÃ©sidu
LayerNorm
    â†“
Feed-Forward (MLP)
    â†“ + connexion rÃ©sidu
```

## ğŸ“š RÃ©fÃ©rences

- "Build a Large Language Model from Scratch" - Sebastian Raschka
- [OpenAI GPT-2](https://openai.com/blog/better-language-models/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al.

## ğŸ“ Notes d'implÃ©mentation

### Tokenization flexible
Le pipeline supporte deux approches :
1. **tiktoken** (par dÃ©faut) : Tokenization BPE compatible OpenAI
2. **SimpleTokenizerV2** : Tokenizer personnalisÃ© pour expÃ©rimentation pÃ©dagogique

### Gestion des erreurs
Le code inclut une gestion robuste :
- Try/except pour la compatibility des tokenizers
- Fallback gracieux pour matplotlib (si non installÃ©)
- Gestion des appareils (CPU/GPU)

## âš¡ Performance

- **DurÃ©e d'entraÃ®nement** : ~5 minutes (10 epochs) sur CPU
- **Taille du checkpoint** : ~621 MB (modÃ¨le seul)
- **MÃ©moire requise** : ~2 GB pour entraÃ®nement

## ğŸ¤ CrÃ©dits

ImplÃ©mentation crÃ©Ã©e pour apprentissage pratique de l'architecture GPT et du deep learning.
BasÃ©e sur les principes pÃ©dagogiques du livre de Sebastian Raschka.
