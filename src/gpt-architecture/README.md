<h1 align="center">
    LLM from scratch
</h1>

<h4 align="center"> Une implÃ©mentation complÃ¨te et bien structurÃ©e d'un modÃ¨le avec capacitÃ©s d'entraÃ®nement, d'Ã©valuation et de gÃ©nÃ©ration de texte. ConÃ§u comme POC pour dÃ©montrer la faisabilitÃ© d'une implÃ©mentation LLM from scratch en production. </h4>

<p align="center">
  <a href="#ğŸ“Š-Vue d'ensemble">Vue d'ensemble</a>
  <a href="#ğŸ›ï¸-Architecture du Projet">Architecture du Projet</a>
  <a href="#âœ¨-CaractÃ©ristiques principales">CaractÃ©ristiques principales</a>
  <a href="#ğŸ“-Notes d'implÃ©mentation">Notes d'implÃ©mentation</a>
  <a href="#ğŸ¤ -crÃ©dits">CrÃ©dit</a>
</p>

## ğŸ“Š Vue d'ensemble

- **ModÃ¨le**: GPT-124M (Transformer decoder-only)
- **Tokenization**: tiktoken (BPE) + support tokenizers personnalisÃ©s
- **Framework**: PyTorch avec architecture modulaire
- **StratÃ©gies de gÃ©nÃ©ration**: greedy, temperature, top-k, top-p (nucleus)
- **Documentation**: ComplÃ¨te en franÃ§ais avec docstrings

## ğŸ›ï¸ Architecture du Projet

```
gpt-architecture/
â”œâ”€â”€ core/                          # âš™ï¸ Architecture du modÃ¨le
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                   # GPTModel (classe principale)
â”‚   â”œâ”€â”€ layers.py                  # LayerNorm, GELU, FeedForward, TransformerBlock
â”‚   â”œâ”€â”€ attention.py               # MultiHeadAttention avec masque causal
â”‚   â””â”€â”€ utils.py                   # Utilitaires (text_to_token_ids, calc_loss_*)
â”‚
â”œâ”€â”€ data/                          # ğŸ“¦ Pipeline de donnÃ©es
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                  # GPTDatasetV1, create_dataloader_v1
â”‚   â””â”€â”€ tokenizer.py               # SimpleTokenizerV1, SimpleTokenizerV2
â”‚
â”œâ”€â”€ decoding/                      # ğŸ² StratÃ©gies de gÃ©nÃ©ration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ strategies.py              # softmax_avec_temperature, top_k, top_p, etc.
â”‚   â””â”€â”€ generator.py               # Fonctions generate_text_*
â”‚
â”œâ”€â”€ scripts/                       # ğŸš€ ExÃ©cutables
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                   # EntraÃ®nement complet du modÃ¨le
â”‚   â”œâ”€â”€ infer.py                   # InfÃ©rence avec diffÃ©rentes stratÃ©gies
â”‚   â”œâ”€â”€ demo.py                    # DÃ©monstration des stratÃ©gies cÃ´te Ã  cÃ´te
â”‚   â”œâ”€â”€ demo_topk.py               # DÃ©monstration pÃ©dagogique du top-k
â”‚   â”œâ”€â”€ test_generate_unified.py   # Tests/validation de la fonction `generate()` unifiÃ©e
â”‚   â”œâ”€â”€ messages.py                # Messages partagÃ©s entre scripts
â”‚   â””â”€â”€ visualize.py               # Visualisation de tempÃ©rature (PDF)
â”‚
â”œâ”€â”€ training.py                    # ğŸ“š Utilitaires d'entraÃ®nement (train_model_simple, etc.)
â”œâ”€â”€ config.py                      # âš™ï¸ Configuration centralisÃ©e
â”œâ”€â”€ README.md                      # ğŸ“– Ce fichier
â””â”€â”€ *.pdf                          # Sorties graphiques (ignorÃ©es par git)
```
### Flux forward
```
EntrÃ©e (token IDs)
    â†“
Embedding tokens + positions
    â†“
Dropout
    â†“
N couches Transformer (attention + feed-forward)
    â†“
LayerNorm
    â†“
Projection de sortie
    â†“
Logits (vocab_size)
```

### Attention multi-tÃªtes
- Division en 12 tÃªtes (768 / 12 = 64 dim par tÃªte)
- Masque causal pour prÃ©venir l'attention sur les jetons futurs
- Softmax avec facteur d'Ã©chelle

### Bloc Transformer
```
Attention multi-tÃªtes
    â†“ + connexion rÃ©sidu
LayerNorm
    â†“
Feed-Forward (MLP)
    â†“ + connexion rÃ©sidu
```

## âœ¨ CaractÃ©ristiques principales

### ğŸ” ModularitÃ©
- **SÃ©paration des responsabilitÃ©s**: core (modÃ¨le), data (donnÃ©es), decoding (gÃ©nÃ©ration), scripts (exÃ©cution)
- **Imports explicites**: Facilement repÃ©rable oÃ¹ vient chaque fonction
- **Configuration centralisÃ©e**: `config.py` pour tous les hyperparamÃ¨tres

### ğŸ§  Architecture Transformer
- Embedding + positional encoding
- Multi-head attention avec masque causal
- Couches Feed-Forward (expansion 4x)
- Layer normalization + rÃ©sidus
- 12 couches Ã— 12 tÃªtes (768 dim)

## ğŸ“ Notes d'implÃ©mentation

### Tokenization flexible
Le pipeline supporte deux approches :
1. **tiktoken** (par dÃ©faut) : Tokenization BPE compatible OpenAI
2. **SimpleTokenizerV2** : Tokenizer personnalisÃ© pour expÃ©rimentation pÃ©dagogique

## RÃ©fÃ©rences
- "Build a Large Language Model from Scratch" - Sebastian Raschka
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al.

## ğŸ¤  CrÃ©dits

<table>
    <tr>
        <td align="center">
            <a href="mailto:aurelien.moignet@imt-atlantique.net">
                <img src="https://avatars.githubusercontent.com/u/76565476?v=4" width="100px;" alt="Image de profil" style="border-radius: 100%"/>
                <br />
                <sub><b>AurÃ©lien</b></sub>
            </a>
            <br />
        </td>
        <td align="center">
                <img src="https://avatars.githubusercontent.com/u/5618407?v=4" width="100px;" alt="Image de profil" style="border-radius: 100%"/>
                <br />
                <sub><b>Sebastian Raschka</b></sub>
                <sub><b>J'ai appris la crÃ©ation des llms from scratch grace aux livres <a href="https://www.amazon.fr/Build-Large-Language-Model-Scratch/dp/1633437167">Build a Large Language Model from Scratch</a></b></sub>
            <br />
        </td>
    </tr>
</table>